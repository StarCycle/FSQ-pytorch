{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IChtTdvWSBiM"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DPaCa4snF4NN"
      },
      "outputs": [],
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "bs = 100\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./train_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./test_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "from torch import Tensor, int32\n",
        "from einops import rearrange\n",
        "\n",
        "def round_ste(z: Tensor) -> Tensor:\n",
        "    \"\"\"Round with straight through gradients.\"\"\"\n",
        "    zhat = z.round()\n",
        "    return z + (zhat - z).detach()\n",
        "\n",
        "# main class\n",
        "class FSQ(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        levels: List[int],\n",
        "        eps: float = 1e-3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._eps = eps\n",
        "\n",
        "        _levels = torch.tensor(levels, dtype=int32)\n",
        "        self.register_buffer(\"_levels\", _levels, persistent = False)\n",
        "\n",
        "        _basis = torch.cumprod(torch.tensor([1] + levels[:-1]), dim=0, dtype=int32)\n",
        "        self.register_buffer(\"_basis\", _basis, persistent = False)\n",
        "\n",
        "        self.codebook_dim = len(levels)\n",
        "        self.codebook_size = self._levels.prod().item()\n",
        "\n",
        "        implicit_codebook = self.indices_to_codes(torch.arange(self.codebook_size), project_out = False)\n",
        "        self.register_buffer(\"implicit_codebook\", implicit_codebook, persistent = False)\n",
        "\n",
        "    def bound(self, z: Tensor) -> Tensor:\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self._levels - 1) * (1 - self._eps) / 2\n",
        "        offset = torch.where(self._levels % 2 == 1.0, 0.0, 0.5)\n",
        "        shift = (offset / half_l).tan()\n",
        "        return (z + shift).tanh() * half_l - offset\n",
        "\n",
        "    def quantize(self, z: Tensor) -> Tensor:\n",
        "        \"\"\"Quantizes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "        quantized = round_ste(self.bound(z))\n",
        "        half_width = self._levels // 2 # Renormalize to [-1, 1].\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized: Tensor) -> Tensor:\n",
        "        # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self._levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat: Tensor) -> Tensor:\n",
        "        half_width = self._levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indices(self, zhat: Tensor) -> Tensor:\n",
        "        \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "        assert zhat.shape[-1] == self.codebook_dim\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self._basis).sum(dim=-1).to(int32)\n",
        "\n",
        "    def indices_to_codes(\n",
        "        self,\n",
        "        indices: Tensor,\n",
        "        project_out = True\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Inverse of `codes_to_indices`.\"\"\"\n",
        "        indices = rearrange(indices, '... -> ... 1')\n",
        "        codes_non_centered = (indices // self._basis) % self._levels\n",
        "        codes = self._scale_and_shift_inverse(codes_non_centered)\n",
        "        return codes"
      ],
      "metadata": {
        "id": "nBXm4AV7jgxx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LKwqDHPIGMi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cd1a62-2dc5-4840-a071-6ea779ffa4c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (fsq): FSQ()\n",
              "  (fc1): Linear(in_features=848, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc21): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc22): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc23): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (fc4): Linear(in_features=74, out_features=512, bias=True)\n",
              "  (fc5): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
              "  (label_embedding): Embedding(10, 64)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, nclass, ncond):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fsq = FSQ(levels=[5]*z_dim)\n",
        "\n",
        "        # encoder part\n",
        "        self.fc1 = nn.Linear(x_dim+ncond, h_dim1)\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
        "        self.fc21 = nn.Linear(h_dim2, h_dim2)\n",
        "        self.fc22 = nn.Linear(h_dim2, h_dim2)\n",
        "        self.fc23 = nn.Linear(h_dim2, h_dim2)\n",
        "        self.fc3 = nn.Linear(h_dim2, z_dim)\n",
        "        # decoder part\n",
        "        self.fc4 = nn.Linear(z_dim+ncond, h_dim2)\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
        "        # default values for generation\n",
        "        self.label_embedding = nn.Embedding(nclass, ncond)\n",
        "\n",
        "    def encoder(self, x, label):\n",
        "        x = torch.cat((x, label), dim=1)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        h = F.relu(self.fc21(h))\n",
        "        h = F.relu(self.fc22(h))\n",
        "        h = F.relu(self.fc23(h))\n",
        "        return self.fc3(h)\n",
        "\n",
        "    def decoder(self, zhat, label):\n",
        "        zhat = torch.cat((zhat, label), dim=1)\n",
        "        h = F.relu(self.fc4(zhat))\n",
        "        h = F.relu(self.fc5(h))\n",
        "        return F.sigmoid(self.fc6(h))\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        label = self.label_embedding(label)\n",
        "        z = self.encoder(x.view(-1, 784), label)\n",
        "        zhat = self.fsq.quantize(z)\n",
        "        return self.decoder(zhat + (z-zhat).detach(), label)\n",
        "\n",
        "    def generate(self, label):\n",
        "        label = torch.tensor([label]).cuda()\n",
        "        label = self.label_embedding(label)\n",
        "        indices = torch.randint(self.fsq.codebook_dim, size=(label.shape[0],)).cuda()\n",
        "        z = self.fsq.indices_to_codes(indices)\n",
        "        return self.decoder(z, label)\n",
        "\n",
        "# build model\n",
        "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=512, z_dim=10, nclass=10, ncond=64)\n",
        "vae.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kEk_F41MGXvZ"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(vae.parameters(), lr=5e-5)\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    return BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jP1-MixDGbKn"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data = data.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch = vae(data, label)\n",
        "        loss = loss_function(recon_batch, data)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_qu_564dGe_l"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in test_loader:\n",
        "            data = data.cuda()\n",
        "            label = label.cuda()\n",
        "            recon = vae(data,label)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data).item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNKlKjknGhw8",
        "outputId": "2e143b0c-8a58-467b-a79f-6d7265dfe18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 544.783281\n",
            "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 217.573477\n",
            "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 211.140996\n",
            "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 193.554375\n",
            "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 181.028672\n",
            "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 176.143359\n",
            "====> Epoch: 0 Average loss: 226.4922\n",
            "====> Test set loss: 175.5013\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 176.195039\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 164.992285\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 175.863711\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 169.287539\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 167.265566\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 166.888438\n",
            "====> Epoch: 1 Average loss: 169.2466\n",
            "====> Test set loss: 161.4690\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 167.170762\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 164.646777\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 148.421309\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 152.117578\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 147.517227\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 148.683457\n",
            "====> Epoch: 2 Average loss: 154.6595\n",
            "====> Test set loss: 148.4456\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 155.461602\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 148.157441\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 159.381250\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 150.891953\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 145.984199\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 143.716016\n",
            "====> Epoch: 3 Average loss: 145.6273\n",
            "====> Test set loss: 138.6472\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 137.066250\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 141.525098\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 140.427012\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 132.071914\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 128.884336\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 132.136406\n",
            "====> Epoch: 4 Average loss: 135.4672\n",
            "====> Test set loss: 132.5765\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 127.438574\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 130.209805\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 128.272383\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 130.727773\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 127.629619\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 138.021338\n",
            "====> Epoch: 5 Average loss: 131.7057\n",
            "====> Test set loss: 130.2346\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 132.490312\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 134.573672\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 128.744395\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 135.052188\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 127.154619\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 132.043604\n",
            "====> Epoch: 6 Average loss: 129.6040\n",
            "====> Test set loss: 128.2843\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 133.673047\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 125.123301\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 121.557988\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 121.746523\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 133.680605\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 126.296250\n",
            "====> Epoch: 7 Average loss: 126.9178\n",
            "====> Test set loss: 123.7615\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 122.762314\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 118.967627\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 127.625967\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 117.678428\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 123.961318\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 118.059824\n",
            "====> Epoch: 8 Average loss: 121.1807\n",
            "====> Test set loss: 117.6603\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 120.414238\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 115.458398\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 123.007412\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 111.488994\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 121.949609\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 113.591953\n",
            "====> Epoch: 9 Average loss: 117.1927\n",
            "====> Test set loss: 115.1712\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 113.274121\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 113.238301\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 112.082246\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 113.618223\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 115.260625\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 117.332979\n",
            "====> Epoch: 10 Average loss: 115.0906\n",
            "====> Test set loss: 113.4839\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 112.520801\n",
            "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 114.782725\n",
            "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 115.608184\n",
            "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 108.628145\n",
            "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 115.910811\n",
            "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 119.123691\n",
            "====> Epoch: 11 Average loss: 113.5777\n",
            "====> Test set loss: 112.1950\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 121.061523\n",
            "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 113.045156\n",
            "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 109.281836\n",
            "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 115.281680\n",
            "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 111.257227\n",
            "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 113.409824\n",
            "====> Epoch: 12 Average loss: 112.4914\n",
            "====> Test set loss: 111.1905\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 106.526113\n",
            "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 111.769736\n",
            "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 109.741328\n",
            "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 115.599277\n",
            "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 116.242070\n",
            "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 112.333828\n",
            "====> Epoch: 13 Average loss: 111.6014\n",
            "====> Test set loss: 110.0938\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 112.627363\n",
            "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 117.265537\n",
            "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 112.723301\n",
            "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 107.732617\n",
            "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 104.221279\n",
            "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 105.042500\n",
            "====> Epoch: 14 Average loss: 110.4149\n",
            "====> Test set loss: 108.6897\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 106.474980\n",
            "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 106.752813\n",
            "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 113.811152\n",
            "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 100.929453\n",
            "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 110.690029\n",
            "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 109.820273\n",
            "====> Epoch: 15 Average loss: 108.6888\n",
            "====> Test set loss: 106.8542\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 107.871367\n",
            "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 104.098926\n",
            "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 111.419170\n",
            "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 104.629004\n",
            "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 103.734395\n",
            "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 107.680713\n",
            "====> Epoch: 16 Average loss: 106.9886\n",
            "====> Test set loss: 105.4468\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 103.957441\n",
            "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 108.055781\n",
            "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 107.842617\n",
            "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 108.822363\n",
            "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 113.387139\n",
            "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 102.343525\n",
            "====> Epoch: 17 Average loss: 105.7804\n",
            "====> Test set loss: 104.7745\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 109.116670\n",
            "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 107.424863\n",
            "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 101.874648\n",
            "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 103.731709\n",
            "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 99.541514\n",
            "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 106.014629\n",
            "====> Epoch: 18 Average loss: 104.8911\n",
            "====> Test set loss: 103.8191\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 102.850537\n",
            "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 104.049297\n",
            "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 103.866514\n",
            "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 107.673193\n",
            "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 103.823662\n",
            "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 103.119482\n",
            "====> Epoch: 19 Average loss: 104.0332\n",
            "====> Test set loss: 102.8607\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 99.845908\n",
            "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 102.735459\n",
            "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 102.513047\n",
            "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 106.509004\n",
            "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 103.931172\n",
            "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 108.178320\n",
            "====> Epoch: 20 Average loss: 103.0897\n",
            "====> Test set loss: 101.8003\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 103.593516\n",
            "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 106.508848\n",
            "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 103.451152\n",
            "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 105.311289\n",
            "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 103.643203\n",
            "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 96.290107\n",
            "====> Epoch: 21 Average loss: 102.1502\n",
            "====> Test set loss: 101.0608\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 98.098457\n",
            "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 97.258789\n",
            "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 103.580215\n",
            "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 105.616641\n",
            "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 103.238945\n",
            "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 102.059619\n",
            "====> Epoch: 22 Average loss: 101.6003\n",
            "====> Test set loss: 100.8715\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 97.142539\n",
            "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 105.573975\n",
            "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 98.343633\n",
            "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 100.868154\n",
            "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 98.595664\n",
            "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 104.310371\n",
            "====> Epoch: 23 Average loss: 101.5740\n",
            "====> Test set loss: 100.4725\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 99.796865\n",
            "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 103.341689\n",
            "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 103.932285\n",
            "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 96.355625\n",
            "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 89.697520\n",
            "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 99.472246\n",
            "====> Epoch: 24 Average loss: 99.8643\n",
            "====> Test set loss: 98.0502\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 92.194004\n",
            "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 99.957715\n",
            "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 98.338184\n",
            "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 98.595703\n",
            "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 98.142861\n",
            "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 91.379414\n",
            "====> Epoch: 25 Average loss: 97.9006\n",
            "====> Test set loss: 96.6009\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 99.796162\n",
            "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 90.276016\n",
            "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 96.814570\n",
            "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 95.959268\n",
            "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 93.178184\n",
            "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 93.613789\n",
            "====> Epoch: 26 Average loss: 96.7520\n",
            "====> Test set loss: 95.6608\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 98.834512\n",
            "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 93.923281\n",
            "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 95.296455\n",
            "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 91.195332\n",
            "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 94.161152\n",
            "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 94.928047\n",
            "====> Epoch: 27 Average loss: 95.8905\n",
            "====> Test set loss: 94.8379\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 98.399219\n",
            "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 96.817119\n",
            "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 91.815449\n",
            "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 98.636875\n",
            "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 89.151895\n",
            "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 91.330771\n",
            "====> Epoch: 28 Average loss: 95.0848\n",
            "====> Test set loss: 94.0650\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 100.259824\n",
            "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 92.233252\n",
            "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 91.307969\n",
            "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 94.556064\n",
            "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 94.465439\n",
            "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 92.850439\n",
            "====> Epoch: 29 Average loss: 94.4869\n",
            "====> Test set loss: 93.5236\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 97.287158\n",
            "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 92.004971\n",
            "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 99.973213\n",
            "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 93.137275\n",
            "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 100.654619\n",
            "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 92.486211\n",
            "====> Epoch: 30 Average loss: 93.8347\n",
            "====> Test set loss: 92.8164\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 94.230479\n",
            "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 97.119678\n",
            "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 93.331494\n",
            "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 98.011934\n",
            "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 94.836113\n",
            "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 96.566211\n",
            "====> Epoch: 31 Average loss: 93.3187\n",
            "====> Test set loss: 92.3130\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 93.154229\n",
            "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 93.145166\n",
            "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 90.665674\n",
            "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 92.896094\n",
            "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 89.966074\n",
            "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 86.278535\n",
            "====> Epoch: 32 Average loss: 92.7969\n",
            "====> Test set loss: 91.9944\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 92.421621\n",
            "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 98.453398\n",
            "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 95.114043\n",
            "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 91.681133\n",
            "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 93.117793\n",
            "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 90.136367\n",
            "====> Epoch: 33 Average loss: 92.3711\n",
            "====> Test set loss: 91.5562\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 88.535293\n",
            "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 91.223027\n",
            "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 91.365635\n",
            "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 85.891426\n",
            "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 93.189473\n",
            "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 90.643105\n",
            "====> Epoch: 34 Average loss: 91.9587\n",
            "====> Test set loss: 91.0443\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 89.667510\n",
            "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 85.860088\n",
            "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 87.194844\n",
            "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 87.621865\n",
            "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 94.279150\n",
            "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 91.971963\n",
            "====> Epoch: 35 Average loss: 91.5675\n",
            "====> Test set loss: 91.2793\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 88.463291\n",
            "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 90.366035\n",
            "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 90.208828\n",
            "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 90.350000\n",
            "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 98.577510\n",
            "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 88.733750\n",
            "====> Epoch: 36 Average loss: 91.2592\n",
            "====> Test set loss: 90.6868\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 97.075693\n",
            "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 85.805137\n",
            "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 85.919258\n",
            "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 91.433262\n",
            "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 91.526182\n",
            "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 90.798320\n",
            "====> Epoch: 37 Average loss: 90.9246\n",
            "====> Test set loss: 90.3857\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 89.897295\n",
            "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 86.507188\n",
            "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 83.561006\n",
            "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 93.794004\n",
            "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 85.645566\n",
            "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 90.311582\n",
            "====> Epoch: 38 Average loss: 90.7260\n",
            "====> Test set loss: 90.0359\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 92.298750\n",
            "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 90.865771\n",
            "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 88.326602\n",
            "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 89.875527\n",
            "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 91.394600\n",
            "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 88.169629\n",
            "====> Epoch: 39 Average loss: 90.4215\n",
            "====> Test set loss: 90.0135\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 96.439941\n",
            "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 92.671865\n",
            "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 88.551602\n",
            "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 87.415625\n",
            "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 94.055020\n",
            "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 88.613105\n",
            "====> Epoch: 40 Average loss: 90.1506\n",
            "====> Test set loss: 89.8997\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 90.769473\n",
            "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 92.272832\n",
            "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 94.823809\n",
            "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 89.478887\n",
            "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 92.991143\n",
            "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 89.603418\n",
            "====> Epoch: 41 Average loss: 89.9493\n",
            "====> Test set loss: 89.7132\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 90.018438\n",
            "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 88.518760\n",
            "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 89.163281\n",
            "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 91.828477\n",
            "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 88.310791\n",
            "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 93.246162\n",
            "====> Epoch: 42 Average loss: 89.8546\n",
            "====> Test set loss: 89.5855\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 91.513818\n",
            "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 87.545020\n",
            "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 91.247695\n",
            "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 88.937305\n",
            "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 86.670752\n",
            "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 90.898770\n",
            "====> Epoch: 43 Average loss: 89.5960\n",
            "====> Test set loss: 89.3584\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 94.297617\n",
            "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 95.121201\n",
            "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 92.469102\n",
            "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 88.100918\n",
            "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 89.953125\n",
            "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 88.077090\n",
            "====> Epoch: 44 Average loss: 89.3811\n",
            "====> Test set loss: 89.0135\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 88.443936\n",
            "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 92.766621\n",
            "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 84.611631\n",
            "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 88.006738\n",
            "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 92.055820\n",
            "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 89.903018\n",
            "====> Epoch: 45 Average loss: 89.3067\n",
            "====> Test set loss: 88.8462\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 92.177373\n",
            "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 96.807266\n",
            "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 91.739932\n",
            "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 92.978330\n",
            "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 86.460391\n",
            "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 96.559619\n",
            "====> Epoch: 46 Average loss: 89.1147\n",
            "====> Test set loss: 89.5483\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 86.300000\n",
            "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 84.961738\n",
            "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 88.131416\n",
            "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 89.124404\n",
            "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 91.757246\n",
            "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 91.872822\n",
            "====> Epoch: 47 Average loss: 89.0319\n",
            "====> Test set loss: 89.0590\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 92.079473\n",
            "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 85.542793\n",
            "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 86.716973\n",
            "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 92.595762\n",
            "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 94.584648\n",
            "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 90.128672\n",
            "====> Epoch: 48 Average loss: 88.9849\n",
            "====> Test set loss: 88.9863\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 88.857744\n",
            "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 94.163320\n",
            "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 85.964785\n",
            "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 88.397109\n",
            "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 93.819531\n",
            "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 89.119658\n",
            "====> Epoch: 49 Average loss: 88.8248\n",
            "====> Test set loss: 88.7813\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50):\n",
        "    train(epoch)\n",
        "    test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "DKV1q4jTRhhq",
        "outputId": "7b7fc11f-ffc8-4556-9398-70fc9e293c2e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from einops import rearrange\n",
        "\n",
        "img, label = test_dataset[9]\n",
        "with torch.no_grad():\n",
        "  gen_img = vae.generate(label)\n",
        "  img = rearrange(img, 'c h w -> h w c')\n",
        "  gen_img = rearrange(gen_img.cpu(), 'c (h w) -> h w c', h=img.shape[0])\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(img)\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(gen_img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}